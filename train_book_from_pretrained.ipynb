{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from helpers.data import get_dataloaders\n",
    "from helpers.train import TrainingManager\n",
    "from helpers.loss_accuracy import accuracy\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_Conv2d = partial(nn.Conv2d, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "_BN2d = nn.BatchNorm2d\n",
    "_act = partial(nn.ReLU, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, testloader = get_dataloaders('cifar10_resnet110_output', 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "is_trial = True\n",
    "load = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.seq = nn.Sequential(_Conv2d(in_planes, planes, stride=stride), _BN2d(planes), _act(), _Conv2d(planes, planes), _BN2d(planes))\n",
    "        self.shortcut = \\\n",
    "            nn.Sequential(_Conv2d(in_planes, planes, kernel_size=1, stride=stride), _BN2d(planes)) if stride != 1 or in_planes != planes else nn.Sequential() \n",
    "    def forward(self, x):\n",
    "        return F.relu(self.seq(x) + self.shortcut(x))\n",
    "\n",
    "class WeightChangerBlock(nn.Module):\n",
    "    def __init__(self, in_c, w_size):\n",
    "        super(WeightChangerBlock, self).__init__()\n",
    "        self.w_size = w_size\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.avgpool = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Flatten())\n",
    "        self.maxpool = nn.Sequential(nn.AdaptiveMaxPool2d(1), nn.Flatten())\n",
    "        self.w_creator = nn.Linear(in_c*2, np.prod(w_size))\n",
    "    def forward(self, last_output):\n",
    "        features = self.tanh(torch.cat((self.avgpool(last_output), self.maxpool(last_output)), 1))\n",
    "        return self.w_creator(features).reshape((-1, *self.w_size))\n",
    "        \n",
    "class BasicBlockFB(nn.Module):\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlockFB, self).__init__()\n",
    "        assert(in_planes == planes)\n",
    "        self.stride = stride\n",
    "        self.weight_changer_block = WeightChangerBlock(planes, (in_planes, planes, 3, 3))\n",
    "        self.seq = nn.Sequential(_BN2d(planes), _act(), _Conv2d(planes, planes), _BN2d(planes))\n",
    "        self.shortcut = \\\n",
    "            nn.Sequential(_Conv2d(in_planes, planes, kernel_size=1, stride=stride), _BN2d(planes)) if stride != 1 or in_planes != planes else nn.Sequential() \n",
    "\n",
    "    def metaconv(self, x, w):\n",
    "        '''\n",
    "        Forward pass of a meta convolution layer.\n",
    "        Note that we do not conv all batch with the same set of conv weights.\n",
    "        The trick is to use group convolutions for convolving each input with its own set of conv weights.\n",
    "        '''\n",
    "        holdx, holdw = x, w\n",
    "        w = w.reshape(w.shape[0] * w.shape[1], *w.shape[2:])\n",
    "        x = x.reshape(1, x.shape[0] * x.shape[1], *x.shape[2:])\n",
    "        out = F.conv2d(x, w, None, stride=self.stride, groups=holdx.shape[0], padding=1)\n",
    "        return out.reshape(holdx.shape[0], holdw.shape[1], holdx.shape[2], holdx.shape[3])\n",
    "    \n",
    "    def forward(self, x, last_output):\n",
    "        w = self.weight_changer_block(last_output)\n",
    "        out = self.metaconv(x, w)\n",
    "        out = F.relu(self.seq(out) + self.shortcut(x), inplace=True)\n",
    "        return out\n",
    "\n",
    "class SuffixModel(nn.Module):\n",
    "    def __init__(self, in_planes, planes, num_loops, num_classes):\n",
    "        super(SuffixModel, self).__init__()\n",
    "        self.num_loops = num_loops\n",
    "        self.block = BasicBlockFB(in_planes, planes)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1), nn.Flatten(),\n",
    "            nn.Linear(planes, num_classes)\n",
    "        )\n",
    "        self.first_last_output = nn.Parameter(torch.zeros(1, 64, 6, 6))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        last_output = self.first_last_output.expand(x.shape[0], -1, -1, -1)\n",
    "        for i in range(self.num_loops):\n",
    "            last_output = self.block(x, last_output)\n",
    "\n",
    "        return self.classifier(last_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SuffixModel(64, 64, 1, 10).to(device)\n",
    "optimizer = Adam(model.parameters(), 2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_name = f'resnet110_pretrained_feedback_1_with_parameter_as_first_inputs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = TrainingManager(trial_name, load=load, is_trial=is_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training trial: \u001b[34mresnet110_pretrained_feedback_100_with_parameter_as_first_inputs\u001b[0m \u001b[31mis_trial\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{tr_loss: 0.80762, tr_acc: 0.88625, te_loss: 0.58180, te_acc: 0.91063}:   0%|          | 152/100000 [00:51<9:21:48,  2.96it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-605c0cd69b61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m          \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m          \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m          accuracy, accuracy, device=device, no_iterations=100000)\n\u001b[0m",
      "\u001b[0;32m~/feedback-loop/helpers/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model, optimizer, trainloader, testloader, criterion_train, criterion_test, acc_train, acc_test, test_iter_each_x_train_iter, mean_range, save_rate, desc_update_rate, no_iterations, device)\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainiter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mtr_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtest_iter_each_x_train_iter\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dl-env/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dl-env/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tm.train(model, optimizer, \n",
    "         trainloader, testloader, \n",
    "         CrossEntropyLoss(), CrossEntropyLoss(), \n",
    "         accuracy, accuracy, device=device, no_iterations=100000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
